{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f6d020-5d06-4105-b1cc-34f18ed4920c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader as PYG_DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GCN\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbbfac8-4e59-4ee9-9501-37afaa357cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置种子\n",
    "SEED = 5\n",
    "random.seed(SEED)        # Python的随机库\n",
    "np.random.seed(SEED)     # NumPy库\n",
    "torch.manual_seed(SEED)  # CPU上的PyTorch操作\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)           # 当前GPU上的PyTorch操作\n",
    "    torch.cuda.manual_seed_all(SEED)       # 所有GPU上的PyTorch操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918af9d9-c93d-471b-aad0-14b1b73d04f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(recommended_variables, k, true_logged_variables):\n",
    "    relevant = [var for var in recommended_variables[:k] if var in true_logged_variables]\n",
    "    precision = len(relevant) / k\n",
    "    return precision\n",
    "\n",
    "def calculate_average_precision(recommended_variables, true_logged_variables):\n",
    "    precisions = [calculate_precision_at_k(recommended_variables, k + 1, true_logged_variables)\n",
    "                  for k in range(len(recommended_variables)) if recommended_variables[k] in true_logged_variables]\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    average_precision = sum(precisions) / len(true_logged_variables)\n",
    "    return average_precision\n",
    "\n",
    "def calculate_map(recommendations, true_logged_variables_list):\n",
    "    average_precisions = [calculate_average_precision(recs, true_vars)\n",
    "                          for recs, true_vars in zip(recommendations, true_logged_variables_list)]\n",
    "    map_score = sum(average_precisions) / len(average_precisions)\n",
    "    return map_score\n",
    "\n",
    "def top_k_acc(ground_truth_list, pred_list, k):\n",
    "    pred_top_k = pred_list[0:k]\n",
    "    # 遍历 pred_top_k 查询预测的变量是否在ground_truth_list中\n",
    "    for pred_var1 in pred_top_k:\n",
    "        if pred_var1 in ground_truth_list:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def calculate_mrr(true_labels, predicted_lst):\n",
    "    \"\"\" 计算 MRR \"\"\"\n",
    "    reciprocal_rank_list = []\n",
    "    # 查询 true_labels 中每个变量在 predicted_lst 中的位置\n",
    "    for var_name in true_labels:\n",
    "        # 如果变量在预测的列表里\n",
    "        if var_name in predicted_lst:\n",
    "            # 查询 var_name 在 predicted_lst 列表中的第一个位置\n",
    "            first_occurrence_index = predicted_lst.index(var_name)\n",
    "            # 求该变量索引的倒数值\n",
    "            var_result = 1 / (first_occurrence_index + 1)\n",
    "            # print(\"var_result:\", var_result)\n",
    "            reciprocal_rank_list.append(var_result)\n",
    "        # 如果变量不在预测的列表中\n",
    "        elif var_name not in predicted_lst:\n",
    "            var_result = 0\n",
    "            reciprocal_rank_list.append(var_result)\n",
    "    return max(reciprocal_rank_list)\n",
    "\n",
    "def find_top_three_indices(lst):\n",
    "    # 创建列表,其中列表的元素是元组，每个元组包含预测值和对应的索引\n",
    "    indexed_lst = [(index1, value) for index1, value in enumerate(lst)]\n",
    "    # print(\"排序前:\", indexed_lst)\n",
    "    # 对indexed_lst 按每个元组的第1个子元素从大到小顺序排序\n",
    "    indexed_lst.sort(reverse=True, key=lambda x: x[1])\n",
    "    # print(\"排序后:\", indexed_lst)\n",
    "    return indexed_lst\n",
    "\n",
    "class MaskedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MaskedBCEWithLogitsLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets, mask):\n",
    "        # 应用 Sigmoid 激活函数\n",
    "        logits = torch.sigmoid(logits)\n",
    "\n",
    "        # 计算二进制交叉熵损失\n",
    "        loss = - (targets * torch.log(logits + 1e-7) + (1 - targets) * torch.log(1 - logits + 1e-7))\n",
    "\n",
    "        # 将损失张量与掩码相乘以过滤掉不想考虑的样本的损失\n",
    "        loss = loss * mask\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            # 计算平均损失\n",
    "            loss = torch.sum(loss) / torch.sum(mask)\n",
    "        elif self.reduction == 'sum':\n",
    "            # 计算总损失\n",
    "            loss = torch.sum(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923f2491-54e6-4211-bbe1-67b439e0feef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytecode_train_dataset_size: 647\n",
      "Bytecode_val_dataset_size: 80\n",
      "Bytecode_test_dataset_size: 82\n"
     ]
    }
   ],
   "source": [
    "class Bytecode_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pkl_path):\n",
    "        # 加载pkl文件\n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            pkl_data_list = pickle.load(file)\n",
    "        self.pkl_data_list = pkl_data_list\n",
    "        self.pkl_data_len = len(self.pkl_data_list)\n",
    "        # print(\"pkl_path:\", pkl_path)\n",
    "        # print(\"pkl_data_len:\", self.pkl_data_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pkl_data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取第 index 个样本的数据\n",
    "        train_data = self.pkl_data_list[idx]\n",
    "        # print(\"train_data:\",train_data)\n",
    "        return (train_data['bytecode_tensor'], train_data['label_tensor'],\n",
    "                train_data['var_mask_tensor'], train_data['train_name'])\n",
    "\n",
    "# 实例化 Bytecode 数据集\n",
    "Bytecode_train_dataset = Bytecode_Dataset(\"../hy-tmp/all_data/split_data/train_pkl.pkl\")\n",
    "Bytecode_val_dataset = Bytecode_Dataset(\"../hy-tmp/all_data/split_data/val_pkl.pkl\")\n",
    "Bytecode_test_dataset = Bytecode_Dataset(\"../hy-tmp/all_data/split_data/test_pkl.pkl\")\n",
    "\n",
    "# 实例化 Bytecode 数据集 Dataloader\n",
    "Bytecode_train_loader = DataLoader(Bytecode_train_dataset, batch_size=64, shuffle=False)\n",
    "Bytecode_val_loader = DataLoader(Bytecode_val_dataset, batch_size=1, shuffle=False)\n",
    "Bytecode_test_loader = DataLoader(Bytecode_test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 获取 Bytecode 数据集长度\n",
    "print(\"Bytecode_train_dataset_size:\", len(Bytecode_train_dataset))\n",
    "print(\"Bytecode_val_dataset_size:\", len(Bytecode_val_dataset))\n",
    "print(\"Bytecode_test_dataset_size:\", len(Bytecode_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec39b478-f68c-43de-9ab3-660ec07fc5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test LSTM output.shape: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# 创建 LSTM 网络模型\n",
    "class LSTM_model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=768,\n",
    "                            hidden_size=128,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2 * 128, 128)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # LSTM层\n",
    "        lstm_output, (h, c) = self.lstm(inputs)\n",
    "\n",
    "        # cat层\n",
    "        cat_hidden = torch.cat((h[-2], h[-1]), -1)\n",
    "\n",
    "        # 线性层1\n",
    "        cat_hidden = self.linear1(cat_hidden)\n",
    "\n",
    "        return cat_hidden\n",
    "\n",
    "# 实例化 LSTM 模型\n",
    "model1 = LSTM_model1()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model1.to(device)\n",
    "\n",
    "# LSTM 模型测试\n",
    "test_inputs = torch.ones((4, 550, 768), dtype=torch.float32, device=device)\n",
    "output = model1(test_inputs)\n",
    "print(\"test LSTM output.shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad08dfd4-07e9-49f4-9017-cf91706a49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= 以下是 Var 模型的代码 =================\n",
      "train_dataset_size: 647\n",
      "val_dataset_size: 80\n",
      "test_dataset_size: 82\n"
     ]
    }
   ],
   "source": [
    "print(\"================= 以下是 Var 模型的代码 =================\")\n",
    "class Var_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pkl_path):\n",
    "        # 加载pkl文件\n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            pkl_data_list = pickle.load(file)\n",
    "            self.pkl_data_list = pkl_data_list\n",
    "            self.pkl_data_len = len(self.pkl_data_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pkl_data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取第 index 个样本的数据\n",
    "        train_data = self.pkl_data_list[idx]\n",
    "        # print(\"train_data:\",train_data)\n",
    "        return (train_data['var_node_tensor'], train_data['label_tensor'],\n",
    "                train_data['var_mask_tensor'], train_data['train_name'])\n",
    "\n",
    "# 实例化 Bytecode 数据集\n",
    "Var_train_dataset = Var_Dataset(\"../hy-tmp/all_data/split_data/train_pkl.pkl\")\n",
    "Var_val_dataset = Var_Dataset(\"../hy-tmp/all_data/split_data/val_pkl.pkl\")\n",
    "Var_test_dataset = Var_Dataset(\"../hy-tmp/all_data/split_data/test_pkl.pkl\")\n",
    "\n",
    "print(\"train_dataset_size:\", len(Var_train_dataset))\n",
    "print(\"val_dataset_size:\", len(Var_val_dataset))\n",
    "print(\"test_dataset_size:\", len(Var_test_dataset))\n",
    "\n",
    "# 实例化 Bytecode 数据集 Dataloader\n",
    "Var_train_loader = DataLoader(Var_train_dataset, batch_size=64, shuffle=False)\n",
    "Var_val_loader = DataLoader(Var_val_dataset, batch_size=1, shuffle=False)\n",
    "Var_test_loader = DataLoader(Var_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bb57d4-1a66-4c21-a204-fd3949150ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var_model(\n",
       "  (conv): Conv1d(768, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Var_model(nn.Module):\n",
    "    def __init__(self, input_dim=768, sentence_length=20, out_channels=128, kernel_size=3):\n",
    "        super(Var_model, self).__init__()\n",
    "\n",
    "        # 1D卷积，卷积核的大小为3，步长为1，仅在句子长度上滑动\n",
    "        self.conv = nn.Conv1d(in_channels=input_dim,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=1,\n",
    "                              padding=1)  # 保持长度\n",
    "\n",
    "        # 全局平均池化层\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # 全连接层用于输出\n",
    "        # self.fc = nn.Linear(out_channels, 20)  # 输出大小可根据需要修改\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入维度为 (batch_size, sentence_length, input_dim)\n",
    "        # 需要转换为 (batch_size, input_dim, sentence_length) 以便于 Conv1d 操作\n",
    "        x = x.permute(0, 2, 1)  # 变为 (batch_size, input_dim, sentence_length)\n",
    "\n",
    "        # 经过一维卷积层\n",
    "        x = self.conv(x)  # 维度变为 (batch_size, out_channels, sentence_length)\n",
    "\n",
    "        # 全局平均池化\n",
    "        x = self.global_pool(x)  # 变为 (batch_size, out_channels, 1)\n",
    "        x = x.squeeze(-1)  # 去掉最后一个维度，变为 (batch_size, out_channels)\n",
    "\n",
    "        # 全连接层\n",
    "        # output = self.fc(x)  # 输出维度为 (batch_size, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 实例化 LSTM 模型\n",
    "model2 = Var_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121979ee-dd50-4197-ae71-49631891cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================以下是 CFG 模型的代码=================\n"
     ]
    }
   ],
   "source": [
    "print(\"=================以下是 CFG 模型的代码=================\")\n",
    "# 定义GCN_Dataset\n",
    "class CFG_Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(CFG_Dataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        # print(\"--- init 开始执行 ----\")\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    # 在 process 方法中，需要将原始数据集处理成图数据结构，其中每个图用一个 Data 对象表示\n",
    "    # 所有处理好的 Data 对象应该可以被索引，因此通常需要将 Data 存储在一个列表中。\n",
    "    def process(self):\n",
    "        # print(\"--- process 开始执行 ----\")\n",
    "        print(\"self.raw_paths[0]:\", self.raw_paths[0])  # 原始文件存放的路径\n",
    "        print(\"self.process_paths[0]:\", self.processed_paths[0])  # 处理后的文件存放的路径\n",
    "        pyg_data_list = []\n",
    "        # 读取 raw 文件夹下的 pickle 文件\n",
    "        with open(self.raw_paths[0], mode=\"rb\") as pkl_file:\n",
    "            pkl_data_list = pickle.load(pkl_file)\n",
    "            print(\"数据集长度为:\", len(pkl_data_list))\n",
    "            for data_index, pkl_data in enumerate(pkl_data_list):\n",
    "                print(\"===================== data_index:\", data_index, \"=====================\")\n",
    "                # print(\"all_data:\", pkl_data)\n",
    "                pyg_data = Data(x=pkl_data[\"node_tensor\"],\n",
    "                                edge_index=pkl_data[\"edge_tensor\"],\n",
    "                                y=pkl_data[\"label_tensor\"],\n",
    "                                label_mask=pkl_data[\"var_mask_tensor\"],\n",
    "                                train_name=pkl_data[\"train_name\"])\n",
    "                # print(\"pyg_data:\", pyg_data)\n",
    "                pyg_data_list.append(pyg_data)\n",
    "        data, slices = self.collate(pyg_data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    # 返回原始文件地址, 调用 self.raw_paths[0]获取原始数据文件地址\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # print(\"--- processed_file_names 开始执行 ---\")\n",
    "        return [\"graph_data.pkl\"]\n",
    "\n",
    "    # 返回处理后的文件地址, 调用self.processed_paths[0]获取处理后的文件地址\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # print(\"--- processed_file_names 开始执行 ---\")\n",
    "        return [\"processed_data.pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99da186d-0e4e-47bd-80b0-41dc94cbc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_train_data_size: 647\n",
      "GCN_val_data_size: 80\n",
      "GCN_test_data_size: 82\n"
     ]
    }
   ],
   "source": [
    "# 初始化GCN数据集\n",
    "CFG_train_dataset = CFG_Dataset(root='../hy-tmp/all_data/graph_train')\n",
    "CFG_val_dataset = CFG_Dataset(root='../hy-tmp/all_data/graph_val')\n",
    "CFG_test_dataset = CFG_Dataset(root='../hy-tmp/all_data/graph_test')\n",
    "\n",
    "# GCN_DataLoader 实例化\n",
    "CFG_train_loader = PYG_DataLoader(CFG_train_dataset, batch_size=64, shuffle=False)\n",
    "CFG_val_loader = PYG_DataLoader(CFG_val_dataset, batch_size=1, shuffle=False)\n",
    "CFG_test_loader = PYG_DataLoader(CFG_test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 获取数据集样本数\n",
    "print(\"GCN_train_data_size:\", len(CFG_train_dataset))\n",
    "print(\"GCN_val_data_size:\", len(CFG_val_dataset))\n",
    "print(\"GCN_test_data_size:\", len(CFG_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18da669-614a-4820-9a1f-cc6933a81801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN_model(\n",
       "  (conv1): GCNConv(768, 128)\n",
       "  (conv2): GCNConv(128, 128)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 CFG 网络模型\n",
    "class GCN_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(768, 128)\n",
    "        self.conv2 = GCNConv(128, 128)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # 全局池化层\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 实例化GCN模型\n",
    "model3 = GCN_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model3.to(device)\n",
    "\n",
    "# # GCN 网络模型测试\n",
    "# gcn_x = torch.randn((4, 768), dtype=torch.float, device=device)\n",
    "# edge_index = torch.tensor([[0, 1, 2, 3, 2], [1, 2, 3, 0, 0]], dtype=torch.long, device=device)\n",
    "# batch = torch.tensor([0, 0, 0, 0], dtype=torch.long, device=device)\n",
    "# output = model3(gcn_x, edge_index, batch)\n",
    "# print(\"test GCN output.shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce27dfb-3c7c-48f2-acbd-5c6e57ae29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.Bytecode_cell = LSTM_model1()  # 输出形状: [batchsize, 128]\n",
    "        self.Var_cell = Var_model()        # 输出形状: [batchsize, 128]\n",
    "        self.CFG_cell = GCN_model()        # 输出形状: [batchsize, 128]\n",
    "\n",
    "        # 一维卷积层：输入3通道 -> 输出128通道\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=3, padding=1)  # [batchsize, 128, 128]\n",
    "\n",
    "        # 全连接分类层\n",
    "        self.fc = nn.Linear(64 * 128, 20)  # 分类器: 20分类\n",
    "\n",
    "    def forward(self, lstm_input1, lstm_input2, gcn_input1, gcn_edge_index1, gcn_batch1):\n",
    "        # 子模块特征提取\n",
    "        bytecode_features = self.Bytecode_cell(lstm_input1)  # [batchsize, 128]\n",
    "        var_features      = self.Var_cell(lstm_input2)            # [batchsize, 128]\n",
    "        cfg_features      = self.CFG_cell(gcn_input1, gcn_edge_index1, gcn_batch1)  # [batchsize, 128]\n",
    "\n",
    "        # 堆叠为3通道特征\n",
    "        feature_map = torch.stack([bytecode_features, var_features, cfg_features], dim=1)  # [batchsize, 3, 128]\n",
    "\n",
    "        # 单层一维卷积处理\n",
    "        x = F.relu(self.conv1(feature_map)) # [batchsize, 128, 128]\n",
    "        x = x.view(x.size(0), -1)  # [batchsize, 128 * 128]      \n",
    "\n",
    "        # 分类预测\n",
    "        output = self.fc(x)  # [batchsize, 20]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4802e5-f6a7-43af-82d8-c23dfd92b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FusionModel(\n",
       "  (Bytecode_cell): LSTM_model1(\n",
       "    (lstm): LSTM(768, 128, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (Var_cell): Var_model(\n",
       "    (conv): Conv1d(768, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (CFG_cell): GCN_model(\n",
       "    (conv1): GCNConv(768, 128)\n",
       "    (conv2): GCNConv(128, 128)\n",
       "  )\n",
       "  (conv1): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc): Linear(in_features=8192, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化网络模型\n",
    "fusion_model = FusionModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "fusion_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22289f67-b32a-403b-aca2-19bcab54c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失和优化器\n",
    "criterion = MaskedBCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e780861-be50-4f56-af86-9bbb5687beaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 第 0 轮训练开始 ============\n",
      "train_loss_sum: 7.068459391593933\n",
      "val_loss_sum: 54.06010049581528\n",
      "val_acc: 0.4717708333333334\n",
      "val_mrr: 0.6965972222222223\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6407508229617608\n",
      "模型已经保存\n",
      "============ 第 1 轮训练开始 ============\n",
      "train_loss_sum: 6.685316234827042\n",
      "val_loss_sum: 51.82493543624878\n",
      "val_acc: 0.4717708333333334\n",
      "val_mrr: 0.6991666666666668\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.6444992801316333\n",
      "模型已经保存\n",
      "============ 第 2 轮训练开始 ============\n",
      "train_loss_sum: 6.608028769493103\n",
      "val_loss_sum: 51.01052215695381\n",
      "val_acc: 0.47427083333333336\n",
      "val_mrr: 0.6937847222222224\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.636360973993327\n",
      "============ 第 3 轮训练开始 ============\n",
      "train_loss_sum: 6.53603333234787\n",
      "val_loss_sum: 50.338016003370285\n",
      "val_acc: 0.47427083333333336\n",
      "val_mrr: 0.6960805860805863\n",
      "val_top1_acc: 0.5375\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.635841361661215\n",
      "============ 第 4 轮训练开始 ============\n",
      "train_loss_sum: 6.485296428203583\n",
      "val_loss_sum: 49.78828448057175\n",
      "val_acc: 0.4680208333333334\n",
      "val_mrr: 0.6883730158730161\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6338092213478245\n",
      "============ 第 5 轮训练开始 ============\n",
      "train_loss_sum: 6.45547416806221\n",
      "val_loss_sum: 49.53435096144676\n",
      "val_acc: 0.4805208333333334\n",
      "val_mrr: 0.6835371572871576\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.635010970987074\n",
      "============ 第 6 轮训练开始 ============\n",
      "train_loss_sum: 6.428417921066284\n",
      "val_loss_sum: 49.30831629037857\n",
      "val_acc: 0.4805208333333334\n",
      "val_mrr: 0.6838591269841272\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.634597911824015\n",
      "============ 第 7 轮训练开始 ============\n",
      "train_loss_sum: 6.397624313831329\n",
      "val_loss_sum: 49.13098320364952\n",
      "val_acc: 0.4909375000000001\n",
      "val_mrr: 0.6879662698412701\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.725\n",
      "val_map: 0.6361698815209846\n",
      "============ 第 8 轮训练开始 ============\n",
      "train_loss_sum: 6.365038096904755\n",
      "val_loss_sum: 48.93482995033264\n",
      "val_acc: 0.4721875000000001\n",
      "val_mrr: 0.6839191017316019\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6335468957104988\n",
      "============ 第 9 轮训练开始 ============\n",
      "train_loss_sum: 6.328900843858719\n",
      "val_loss_sum: 48.73760271072388\n",
      "val_acc: 0.4851041666666667\n",
      "val_mrr: 0.6838095238095239\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.725\n",
      "val_map: 0.6400584397220428\n",
      "============ 第 10 轮训练开始 ============\n",
      "train_loss_sum: 6.2884611785411835\n",
      "val_loss_sum: 48.54428908228874\n",
      "val_acc: 0.48302083333333334\n",
      "val_mrr: 0.6893303571428573\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.6464156111964018\n",
      "模型已经保存\n",
      "============ 第 11 轮训练开始 ============\n",
      "train_loss_sum: 6.243425399065018\n",
      "val_loss_sum: 48.35344907641411\n",
      "val_acc: 0.5132291666666667\n",
      "val_mrr: 0.6906150793650794\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.6547087243332648\n",
      "模型已经保存\n",
      "============ 第 12 轮训练开始 ============\n",
      "train_loss_sum: 6.193233281373978\n",
      "val_loss_sum: 48.132845997810364\n",
      "val_acc: 0.5361458333333333\n",
      "val_mrr: 0.7166567460317461\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6735988272858677\n",
      "模型已经保存\n",
      "============ 第 13 轮训练开始 ============\n",
      "train_loss_sum: 6.145374953746796\n",
      "val_loss_sum: 48.00399896502495\n",
      "val_acc: 0.5486458333333333\n",
      "val_mrr: 0.7249702380952382\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6820654127837031\n",
      "模型已经保存\n",
      "============ 第 14 轮训练开始 ============\n",
      "train_loss_sum: 6.093274295330048\n",
      "val_loss_sum: 47.90554943680763\n",
      "val_acc: 0.5673958333333333\n",
      "val_mrr: 0.7268303571428572\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6871738720327875\n",
      "模型已经保存\n",
      "============ 第 15 轮训练开始 ============\n",
      "train_loss_sum: 6.067362070083618\n",
      "val_loss_sum: 48.11573466658592\n",
      "val_acc: 0.5381101190476191\n",
      "val_mrr: 0.709172077922078\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.6574257374569876\n",
      "============ 第 16 轮训练开始 ============\n",
      "train_loss_sum: 6.045708417892456\n",
      "val_loss_sum: 48.34961575269699\n",
      "val_acc: 0.5547767857142857\n",
      "val_mrr: 0.7342857142857144\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6786543551934179\n",
      "============ 第 17 轮训练开始 ============\n",
      "train_loss_sum: 6.004728019237518\n",
      "val_loss_sum: 48.734290704131126\n",
      "val_acc: 0.5297767857142858\n",
      "val_mrr: 0.7125054112554114\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6551442429098681\n",
      "============ 第 18 轮训练开始 ============\n",
      "train_loss_sum: 5.97254291176796\n",
      "val_loss_sum: 48.51182709634304\n",
      "val_acc: 0.5147767857142858\n",
      "val_mrr: 0.7099161255411256\n",
      "val_top1_acc: 0.5375\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6511261568292819\n",
      "============ 第 19 轮训练开始 ============\n",
      "train_loss_sum: 5.939393728971481\n",
      "val_loss_sum: 48.16976410150528\n",
      "val_acc: 0.5418601190476191\n",
      "val_mrr: 0.7314880952380953\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6693664463430091\n",
      "============ 第 20 轮训练开始 ============\n",
      "train_loss_sum: 5.891842246055603\n",
      "val_loss_sum: 47.97981006652117\n",
      "val_acc: 0.5832291666666667\n",
      "val_mrr: 0.7413690476190478\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6873118230149481\n",
      "模型已经保存\n",
      "============ 第 21 轮训练开始 ============\n",
      "train_loss_sum: 5.8389357924461365\n",
      "val_loss_sum: 47.82455375790596\n",
      "val_acc: 0.5982291666666666\n",
      "val_mrr: 0.7553273809523811\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6957083775599402\n",
      "模型已经保存\n",
      "============ 第 22 轮训练开始 ============\n",
      "train_loss_sum: 5.781752556562424\n",
      "val_loss_sum: 47.73800025880337\n",
      "val_acc: 0.595625\n",
      "val_mrr: 0.7674107142857144\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.7013405101842602\n",
      "模型已经保存\n",
      "============ 第 23 轮训练开始 ============\n",
      "train_loss_sum: 5.739323675632477\n",
      "val_loss_sum: 47.73754597455263\n",
      "val_acc: 0.5940624999999999\n",
      "val_mrr: 0.7538690476190478\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6984900186618938\n",
      "============ 第 24 轮训练开始 ============\n",
      "train_loss_sum: 5.6966469287872314\n",
      "val_loss_sum: 47.55032802000642\n",
      "val_acc: 0.5621875\n",
      "val_mrr: 0.7515625000000001\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6948144121503497\n",
      "============ 第 25 轮训练开始 ============\n",
      "train_loss_sum: 5.643904387950897\n",
      "val_loss_sum: 47.580870505422354\n",
      "val_acc: 0.6105059523809524\n",
      "val_mrr: 0.7692857142857144\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.8125\n",
      "val_map: 0.7064296571484072\n",
      "模型已经保存\n",
      "============ 第 26 轮训练开始 ============\n",
      "train_loss_sum: 5.60600608587265\n",
      "val_loss_sum: 47.71379816532135\n",
      "val_acc: 0.6000892857142858\n",
      "val_mrr: 0.7692857142857144\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.825\n",
      "val_map: 0.7063842511655013\n",
      "============ 第 27 轮训练开始 ============\n",
      "train_loss_sum: 5.5856461226940155\n",
      "val_loss_sum: 48.174657829105854\n",
      "val_acc: 0.5772767857142858\n",
      "val_mrr: 0.7668750000000002\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6985848960067711\n",
      "============ 第 28 轮训练开始 ============\n",
      "train_loss_sum: 5.661754369735718\n",
      "val_loss_sum: 49.01911295950413\n",
      "val_acc: 0.5089434523809524\n",
      "val_mrr: 0.7045833333333335\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.725\n",
      "val_map: 0.6481276712292339\n",
      "============ 第 29 轮训练开始 ============\n",
      "train_loss_sum: 5.660756677389145\n",
      "val_loss_sum: 48.87552027404308\n",
      "val_acc: 0.5360267857142857\n",
      "val_mrr: 0.7110714285714287\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.725\n",
      "val_map: 0.65407531733313\n",
      "============ 第 30 轮训练开始 ============\n",
      "train_loss_sum: 5.7082047164440155\n",
      "val_loss_sum: 47.40473795682192\n",
      "val_acc: 0.5896726190476189\n",
      "val_mrr: 0.7683035714285714\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.7071282693001443\n",
      "模型已经保存\n",
      "============ 第 31 轮训练开始 ============\n",
      "train_loss_sum: 5.592680335044861\n",
      "val_loss_sum: 47.67743969708681\n",
      "val_acc: 0.576875\n",
      "val_mrr: 0.7683387445887447\n",
      "val_top1_acc: 0.65\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.7055173332590612\n",
      "============ 第 32 轮训练开始 ============\n",
      "train_loss_sum: 5.554138779640198\n",
      "val_loss_sum: 48.18830294907093\n",
      "val_acc: 0.5459226190476191\n",
      "val_mrr: 0.7343750000000002\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6698068313766844\n",
      "============ 第 33 轮训练开始 ============\n",
      "train_loss_sum: 5.584954410791397\n",
      "val_loss_sum: 47.13916552811861\n",
      "val_acc: 0.5909375\n",
      "val_mrr: 0.7686363636363637\n",
      "val_top1_acc: 0.6375\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.7077671168530545\n",
      "模型已经保存\n",
      "============ 第 34 轮训练开始 ============\n",
      "train_loss_sum: 5.465902596712112\n",
      "val_loss_sum: 47.9235379435122\n",
      "val_acc: 0.5669791666666666\n",
      "val_mrr: 0.7435565476190478\n",
      "val_top1_acc: 0.6\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6889509699348855\n",
      "============ 第 35 轮训练开始 ============\n",
      "train_loss_sum: 5.4244198352098465\n",
      "val_loss_sum: 47.9775534234941\n",
      "val_acc: 0.5625892857142858\n",
      "val_mrr: 0.7604166666666667\n",
      "val_top1_acc: 0.625\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6913381167443668\n",
      "============ 第 36 轮训练开始 ============\n",
      "train_loss_sum: 5.425877958536148\n",
      "val_loss_sum: 47.75725545734167\n",
      "val_acc: 0.5875892857142857\n",
      "val_mrr: 0.7533630952380953\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.7003630032141063\n",
      "============ 第 37 轮训练开始 ============\n",
      "train_loss_sum: 5.513497531414032\n",
      "val_loss_sum: 47.363748744130135\n",
      "val_acc: 0.5560416666666665\n",
      "val_mrr: 0.7335657051282054\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6829502574971326\n",
      "============ 第 38 轮训练开始 ============\n",
      "train_loss_sum: 5.376958101987839\n",
      "val_loss_sum: 47.42356030270457\n",
      "val_acc: 0.5675\n",
      "val_mrr: 0.7437251984126986\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.693799730940356\n",
      "============ 第 39 轮训练开始 ============\n",
      "train_loss_sum: 5.276458203792572\n",
      "val_loss_sum: 47.22140979766846\n",
      "val_acc: 0.5940624999999999\n",
      "val_mrr: 0.767842261904762\n",
      "val_top1_acc: 0.65\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.7129997970779222\n",
      "模型已经保存\n",
      "============ 第 40 轮训练开始 ============\n",
      "train_loss_sum: 5.235330790281296\n",
      "val_loss_sum: 47.24392559379339\n",
      "val_acc: 0.576875\n",
      "val_mrr: 0.7532440476190476\n",
      "val_top1_acc: 0.625\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6999454816017318\n",
      "============ 第 41 轮训练开始 ============\n",
      "train_loss_sum: 5.182003945112228\n",
      "val_loss_sum: 47.360799638554454\n",
      "val_acc: 0.5482291666666665\n",
      "val_mrr: 0.725357142857143\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6858126728595481\n",
      "============ 第 42 轮训练开始 ============\n",
      "train_loss_sum: 5.142397940158844\n",
      "val_loss_sum: 47.38861909508705\n",
      "val_acc: 0.576875\n",
      "val_mrr: 0.7519494047619047\n",
      "val_top1_acc: 0.625\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6961582792207792\n",
      "============ 第 43 轮训练开始 ============\n",
      "train_loss_sum: 5.126730784773827\n",
      "val_loss_sum: 47.54659306816757\n",
      "val_acc: 0.5732291666666666\n",
      "val_mrr: 0.7381894841269842\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6934009627525253\n",
      "============ 第 44 轮训练开始 ============\n",
      "train_loss_sum: 5.112552687525749\n",
      "val_loss_sum: 47.39470394514501\n",
      "val_acc: 0.5607291666666666\n",
      "val_mrr: 0.7383630952380952\n",
      "val_top1_acc: 0.6\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6945970193001443\n",
      "============ 第 45 轮训练开始 ============\n",
      "train_loss_sum: 5.132689252495766\n",
      "val_loss_sum: 47.806238655000925\n",
      "val_acc: 0.5776934523809525\n",
      "val_mrr: 0.7617857142857145\n",
      "val_top1_acc: 0.625\n",
      "val_top2_acc: 0.8125\n",
      "val_map: 0.7008134391650016\n",
      "============ 第 46 轮训练开始 ============\n",
      "train_loss_sum: 5.0687708258628845\n",
      "val_loss_sum: 47.54700543358922\n",
      "val_acc: 0.5794791666666665\n",
      "val_mrr: 0.7282738095238096\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6908935222763348\n",
      "============ 第 47 轮训练开始 ============\n",
      "train_loss_sum: 5.011842682957649\n",
      "val_loss_sum: 47.12280338071287\n",
      "val_acc: 0.5805208333333333\n",
      "val_mrr: 0.7513839285714285\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.7025786661255412\n",
      "============ 第 48 轮训练开始 ============\n",
      "train_loss_sum: 4.971490830183029\n",
      "val_loss_sum: 47.712115234695375\n",
      "val_acc: 0.5794791666666667\n",
      "val_mrr: 0.7420833333333333\n",
      "val_top1_acc: 0.6\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6967826591810968\n",
      "============ 第 49 轮训练开始 ============\n",
      "train_loss_sum: 4.974527135491371\n",
      "val_loss_sum: 48.063418824225664\n",
      "val_acc: 0.5523958333333333\n",
      "val_mrr: 0.7289880952380954\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6814062950937952\n",
      "============ 第 50 轮训练开始 ============\n",
      "train_loss_sum: 4.940130785107613\n",
      "val_loss_sum: 47.942884808406234\n",
      "val_acc: 0.5992708333333332\n",
      "val_mrr: 0.743888888888889\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.7050088947510822\n",
      "============ 第 51 轮训练开始 ============\n",
      "train_loss_sum: 4.847236081957817\n",
      "val_loss_sum: 48.233424133621156\n",
      "val_acc: 0.5648958333333333\n",
      "val_mrr: 0.7371726190476191\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6830872226731602\n",
      "============ 第 52 轮训练开始 ============\n",
      "train_loss_sum: 4.8348947167396545\n",
      "val_loss_sum: 47.9500679820776\n",
      "val_acc: 0.5753125\n",
      "val_mrr: 0.7408333333333335\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6983785173160173\n",
      "============ 第 53 轮训练开始 ============\n",
      "train_loss_sum: 4.80003497004509\n",
      "val_loss_sum: 49.50443062372506\n",
      "val_acc: 0.5701041666666666\n",
      "val_mrr: 0.7330059523809525\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6841591732919857\n",
      "============ 第 54 轮训练开始 ============\n",
      "train_loss_sum: 4.839007943868637\n",
      "val_loss_sum: 51.042057763785124\n",
      "val_acc: 0.5378125\n",
      "val_mrr: 0.7134672619047621\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6559877943237319\n",
      "============ 第 55 轮训练开始 ============\n",
      "train_loss_sum: 4.761251598596573\n",
      "val_loss_sum: 48.75405261479318\n",
      "val_acc: 0.5544791666666666\n",
      "val_mrr: 0.721860119047619\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6788511566558441\n",
      "============ 第 56 轮训练开始 ============\n",
      "train_loss_sum: 4.699287861585617\n",
      "val_loss_sum: 48.62347035855055\n",
      "val_acc: 0.5648958333333333\n",
      "val_mrr: 0.7288888888888889\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7375\n",
      "val_map: 0.6894126984126984\n",
      "============ 第 57 轮训练开始 ============\n",
      "train_loss_sum: 4.700526922941208\n",
      "val_loss_sum: 49.592726041562855\n",
      "val_acc: 0.5451041666666667\n",
      "val_mrr: 0.7174007936507937\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6709766526875902\n",
      "============ 第 58 轮训练开始 ============\n",
      "train_loss_sum: 4.624382250010967\n",
      "val_loss_sum: 51.12448706757277\n",
      "val_acc: 0.5471875\n",
      "val_mrr: 0.7183184523809525\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6637489741161617\n",
      "============ 第 59 轮训练开始 ============\n",
      "train_loss_sum: 4.618938125669956\n",
      "val_loss_sum: 49.96804931201041\n",
      "val_acc: 0.5430208333333333\n",
      "val_mrr: 0.7269642857142858\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6672192122113998\n",
      "============ 第 60 轮训练开始 ============\n",
      "train_loss_sum: 4.563791871070862\n",
      "val_loss_sum: 52.60771832615137\n",
      "val_acc: 0.556860119047619\n",
      "val_mrr: 0.726889880952381\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6700614741161617\n",
      "============ 第 61 轮训练开始 ============\n",
      "train_loss_sum: 4.68773940205574\n",
      "val_loss_sum: 53.22600217536092\n",
      "val_acc: 0.5391517857142857\n",
      "val_mrr: 0.7020833333333334\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6549791328463204\n",
      "============ 第 62 轮训练开始 ============\n",
      "train_loss_sum: 4.88478983938694\n",
      "val_loss_sum: 49.74259638786316\n",
      "val_acc: 0.541235119047619\n",
      "val_mrr: 0.7271371336996337\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7375\n",
      "val_map: 0.6803605396339772\n",
      "============ 第 63 轮训练开始 ============\n",
      "train_loss_sum: 4.777910947799683\n",
      "val_loss_sum: 51.81737285852432\n",
      "val_acc: 0.5398958333333334\n",
      "val_mrr: 0.6996031746031747\n",
      "val_top1_acc: 0.55\n",
      "val_top2_acc: 0.7125\n",
      "val_map: 0.6530711276570653\n",
      "============ 第 64 轮训练开始 ============\n",
      "train_loss_sum: 4.703619047999382\n",
      "val_loss_sum: 48.62147213332355\n",
      "val_acc: 0.5701041666666666\n",
      "val_mrr: 0.7385912698412699\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6892101145382397\n",
      "============ 第 65 轮训练开始 ============\n",
      "train_loss_sum: 4.631555579602718\n",
      "val_loss_sum: 50.003622353076935\n",
      "val_acc: 0.5565625\n",
      "val_mrr: 0.7346130952380953\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6838492514430016\n",
      "============ 第 66 轮训练开始 ============\n",
      "train_loss_sum: 4.5522730350494385\n",
      "val_loss_sum: 48.81811909005046\n",
      "val_acc: 0.5867708333333332\n",
      "val_mrr: 0.7469288003663004\n",
      "val_top1_acc: 0.6\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6994170647061273\n",
      "============ 第 67 轮训练开始 ============\n",
      "train_loss_sum: 4.481190539896488\n",
      "val_loss_sum: 49.75399908237159\n",
      "val_acc: 0.5701041666666666\n",
      "val_mrr: 0.7497916666666666\n",
      "val_top1_acc: 0.6125\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6886779964826839\n",
      "早停: 在连续 20 个 epoch 中验证集损失loss没有改善.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epoch_list = []\n",
    "    train_loss_sum_list = []\n",
    "    total_val_average_acc_list = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # 早停的参数\n",
    "    min_val_loss_sum = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    patience = 20  # 设置早停等待的 epoch 数\n",
    "\n",
    "    for epoch in range(200):\n",
    "        print(f\"============ 第 {epoch} 轮训练开始 ============\")\n",
    "        # 模型训练\n",
    "        fusion_model.train()\n",
    "        train_loss_sum = 0\n",
    "        # batch_size = 0\n",
    "        for Bytecode_data, Var_data, CFG_data in zip(Bytecode_train_loader, Var_train_loader, CFG_train_loader):\n",
    "            # 获取 Bytecode 数据和标签\n",
    "            Bytecode_inputs, targets, targets_mask, train_csv_names = Bytecode_data\n",
    "            Bytecode_inputs, targets, targets_mask = Bytecode_inputs.to(device), targets.to(device), targets_mask.to(device)\n",
    "            csv_name = train_csv_names[0].split(\".txt\")[0]+\".csv\"\n",
    "\n",
    "            # 获取 Var 数据\n",
    "            Var_inputs, var_targets, var_targets_mask, var_train_csv_names = Var_data\n",
    "            Var_inputs = Var_inputs.to(device)\n",
    "\n",
    "            # 获取 CFG 数据\n",
    "            CFG_inputs = CFG_data.x.to(device)\n",
    "            CFG_edge_index = CFG_data.edge_index.to(device)\n",
    "            CFG_batch = CFG_data.batch.to(device)\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = fusion_model(Bytecode_inputs, Var_inputs, CFG_inputs, CFG_edge_index, CFG_batch)\n",
    "            loss = criterion(outputs, targets, targets_mask)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "\n",
    "            # update\n",
    "            optimizer.step()\n",
    "\n",
    "            # 累加loss\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "        print(\"train_loss_sum:\", train_loss_sum)\n",
    "        train_loss_sum_list.append(train_loss_sum)\n",
    "\n",
    "        # 模型验证\n",
    "        with torch.no_grad():\n",
    "            fusion_model.eval()\n",
    "            val_acc_list = []  # 记录当前 epoch 下每个测试样本的准确率 sample_acc\n",
    "            val_mrr_list = []\n",
    "            val_top1_list = []\n",
    "            val_top2_list = []\n",
    "            true_logged_variables_list = []\n",
    "            recommendations_list = []\n",
    "            val_loss_sum = 0\n",
    "\n",
    "            for Bytecode_data, Var_data, CFG_data in zip(Bytecode_val_loader, Var_val_loader, CFG_val_loader):\n",
    "                # 获取Bytecode数据\n",
    "                Bytecode_inputs, targets, targets_mask, train_csv_names = Bytecode_data\n",
    "                Bytecode_inputs, targets, targets_mask = Bytecode_inputs.to(device), targets.to(device), targets_mask.to(device)\n",
    "                csv_name = train_csv_names[0].split('.txt')[0]+\".csv\"\n",
    "\n",
    "                # 获取 Var 数据\n",
    "                Var_inputs, Var_targets, Var_targets_mask, Var_train_csv_names = Var_data\n",
    "                Var_inputs = Var_inputs.to(device)\n",
    "\n",
    "                # 获取 CFG 数据\n",
    "                CFG_inputs = CFG_data.x.to(device)\n",
    "                CFG_edge_index = CFG_data.edge_index.to(device)\n",
    "                CFG_batch = CFG_data.batch.to(device)\n",
    "\n",
    "                # 模型验证\n",
    "                outputs = fusion_model(Bytecode_inputs, Var_inputs, CFG_inputs, CFG_edge_index, CFG_batch)\n",
    "                loss = criterion(outputs, targets, targets_mask)\n",
    "                val_loss_sum += loss.item()\n",
    "                # print(\"outputs.shape:\", outputs.shape)\n",
    "\n",
    "                # 根据 targets_mask 为1的索引,取 output 中有效的预测结果\n",
    "                mask_list = targets_mask.squeeze().tolist()\n",
    "                # print(\"mask_list:\", mask_list)\n",
    "                one_count = mask_list.count(1)  # 统计 mask_list 有效局部变量的个数\n",
    "                outputs_list1 = outputs.squeeze().tolist()[0: one_count]\n",
    "                # print(\"outputs_list1:\", outputs_list1)\n",
    "\n",
    "                # 把截取后的 outputs_list1 按照预测概率大小排序组成 sorted_list\n",
    "                sorted_list = find_top_three_indices(outputs_list1)\n",
    "                # print(\"sorted_list:\", sorted_list)\n",
    "\n",
    "                # 读取 train_label 文件获取预测值对应 index 位置的变量名\n",
    "                label_dir = \"../hy-tmp/all_data/all_label\"\n",
    "                label_path = os.path.join(label_dir, csv_name)\n",
    "                label_csv = pd.read_csv(label_path)\n",
    "\n",
    "                # var_name_list 列表\n",
    "                var_name_list = label_csv['name'].tolist()\n",
    "                # print(\"var_name_list:\", var_name_list)\n",
    "\n",
    "                # label 列表\n",
    "                label_list = label_csv['label'].tolist()\n",
    "                # print(\"label_list:\", label_list)\n",
    "\n",
    "                # 根据 var_name_list 和 label_list 获得日志中记录的变量名\n",
    "                ground_truth_list = []\n",
    "                for index, label in enumerate(label_list):\n",
    "                    if label == 1:\n",
    "                        ground_truth_list.append(var_name_list[index])\n",
    "                true_logged_variables_list.append(ground_truth_list)\n",
    "                # print(\"ground_truth_list:\", ground_truth_list)\n",
    "\n",
    "                # 根据排序后模型的预测结果, 从 var_name_list 中取出对应的变量名\n",
    "                pred_var_list = []\n",
    "                for ele in sorted_list:\n",
    "                    var_index = ele[0]\n",
    "                    pred_var_list.append(var_name_list[var_index])\n",
    "                # print(\"pred_var_list:\", pred_var_list)\n",
    "                recommendations_list.append(pred_var_list)\n",
    "\n",
    "                # 从 pred_var_list 中取跟 ground_truth 中记录变量个数一样的变量\n",
    "                limit_pred_var_list = pred_var_list[0:len(ground_truth_list)]\n",
    "                # print(\"pred_var_list:\", pred_var_list)\n",
    "\n",
    "                # 计算模型预测的acc\n",
    "                correct_count = 0\n",
    "                # 日志中记录的变量个数\n",
    "                total_count = len(ground_truth_list)\n",
    "                for pred_var in limit_pred_var_list:  # 遍历pred_var_list 中每个变量\n",
    "                    if pred_var in ground_truth_list:\n",
    "                        correct_count += 1\n",
    "                pred_acc = correct_count / total_count\n",
    "                # 添加模型预测的准确率\n",
    "                val_acc_list.append(pred_acc)\n",
    "\n",
    "                # 计算当前样本的mrr\n",
    "                mrr_result = calculate_mrr(ground_truth_list, pred_var_list)\n",
    "                val_mrr_list.append(mrr_result)\n",
    "\n",
    "                # 计算该样本的 top1_acc\n",
    "                top1_result = top_k_acc(ground_truth_list, pred_var_list, k=1)\n",
    "                val_top1_list.append(top1_result)\n",
    "\n",
    "                # 计算该样本的 top2_acc\n",
    "                top2_result = top_k_acc(ground_truth_list, pred_var_list, k=2)\n",
    "                val_top2_list.append(top2_result)\n",
    "\n",
    "            # 计算当前epoch下, 模型在验证集下loss_sum\n",
    "            print(\"val_loss_sum:\", val_loss_sum)\n",
    "\n",
    "            # 计算当前epoch下, 模型在验证集下的平均预测准确率\n",
    "            average_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "            print(\"val_acc:\", average_acc)\n",
    "            total_val_average_acc_list.append(average_acc)\n",
    "\n",
    "            # 计算当前epoch下,模型在验证集下的平均mrr\n",
    "            val_mrr = sum(val_mrr_list) / len(val_mrr_list)\n",
    "            print(\"val_mrr:\", val_mrr)\n",
    "\n",
    "            # 计算当前epoch下,模型的top1_acc\n",
    "            val_top1_acc = sum(val_top1_list) / len(val_top1_list)\n",
    "            print(\"val_top1_acc:\", val_top1_acc)\n",
    "\n",
    "            # 计算当前epoch下,模型的top2_acc\n",
    "            val_top2_acc = sum(val_top2_list) / len(val_top2_list)\n",
    "            print(\"val_top2_acc:\", val_top2_acc)\n",
    "\n",
    "            # 计算当前epoch下,模型的map\n",
    "            val_map = calculate_map(recommendations=recommendations_list, true_logged_variables_list=true_logged_variables_list)\n",
    "            print(\"val_map:\", val_map)\n",
    "\n",
    "            # 检查是否有改善\n",
    "            if val_loss_sum < min_val_loss_sum:\n",
    "                min_val_loss_sum = val_loss_sum\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            # 检查是否需要早停\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"早停: 在连续 {patience} 个 epoch 中验证集损失loss没有改善.\")\n",
    "                break\n",
    "\n",
    "            # Save the model if the accuracy is the best\n",
    "            if val_map > best_accuracy:\n",
    "                # 模型保存\n",
    "                model_dir = \"./model_save\"\n",
    "                model_name = \"fusion_model_CNN.pth\"\n",
    "                model_path = os.path.join(model_dir, model_name)\n",
    "                torch.save(fusion_model, model_path)\n",
    "                best_accuracy = val_map\n",
    "                print(\"模型已经保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ab6be0-da00-4287-8ab4-3c3b6d8265ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 模型测试开始Seed:5 ============\n",
      "test_mrr: 0.7819352057156935\n",
      "test_top1_acc: 0.6341463414634146\n",
      "test_top2_acc: 0.8536585365853658\n",
      "test_map: 0.722694218044828\n"
     ]
    }
   ],
   "source": [
    "print(f\"============ 模型测试开始Seed:{SEED} ============\")\n",
    "# 模型测试\n",
    "test_model = torch.load(\"./model_save/fusion_model_CNN.pth\")\n",
    "# 模型验证(评估当前训练模型的性能)\n",
    "test_model.eval()\n",
    "test_acc_list = []  # 统计每个样本的准确率\n",
    "test_mrr_list = []\n",
    "test_top1_list = []\n",
    "test_top2_list = []\n",
    "true_logged_variables_list = []\n",
    "recommendations_list = []\n",
    "with torch.no_grad():\n",
    "    for Bytecode_data, Var_data, CFG_data in zip(Bytecode_test_loader, Var_test_loader, CFG_test_loader):\n",
    "\n",
    "        # 获取 Bytecode 数据\n",
    "        Bytecode_inputs, targets, targets_mask, train_csv_names = Bytecode_data\n",
    "        Bytecode_inputs, targets, targets_mask = Bytecode_inputs.to(device), targets.to(device), targets_mask.to(device)\n",
    "        csv_name = train_csv_names[0].split(\".txt\")[0]+\".csv\"\n",
    "\n",
    "        # 获取 Var 数据\n",
    "        Var_inputs, Var_targets, Var_targets_mask, Var_train_csv_names = Var_data\n",
    "        Var_inputs = Var_inputs.to(device)\n",
    "\n",
    "        # 获取 CFG 数据\n",
    "        CFG_inputs = CFG_data.x.to(device)\n",
    "        CFG_edge_index = CFG_data.edge_index.to(device)\n",
    "        CFG_batch = CFG_data.batch.to(device)\n",
    "\n",
    "        # 模型测试\n",
    "        outputs = test_model(Bytecode_inputs,\n",
    "                             Var_inputs,\n",
    "                             CFG_inputs, CFG_edge_index, CFG_batch)\n",
    "\n",
    "        # 根据 targets_mask 取 outputs 中有效的预测结果\n",
    "        mask_list = targets_mask.squeeze().tolist()\n",
    "        one_count = mask_list.count(1)  # 统计mask_list有效局部变量的个数\n",
    "\n",
    "        # 取 outputs 里的前 one_count 有效的预测结果\n",
    "        outputs_list1 = outputs.squeeze().tolist()[0: one_count]\n",
    "\n",
    "        # 把截取后的 outputs_list1 按照预测概率大小排序组成 sorted_list\n",
    "        sorted_list = find_top_three_indices(outputs_list1)\n",
    "\n",
    "        # 读取 train_label 获取预测值对应 index 位置的变量名\n",
    "        label_dir = \"../hy-tmp/all_data/all_label\"\n",
    "        label_path = os.path.join(label_dir, csv_name)\n",
    "        label_csv = pd.read_csv(label_path)\n",
    "\n",
    "        # var_name_list 列表\n",
    "        var_name_list = label_csv['name'].tolist()\n",
    "\n",
    "        # label 列表\n",
    "        label_list = label_csv['label'].tolist()\n",
    "\n",
    "        # 根据 var_name_list 和 label_list 获得日志中记录的变量名\n",
    "        ground_truth_list = []\n",
    "        for index, label in enumerate(label_list):\n",
    "            if label == 1:\n",
    "                ground_truth_list.append(var_name_list[index])\n",
    "        true_logged_variables_list.append(ground_truth_list)\n",
    "\n",
    "        # 根据模型预测结果, 取出对应的变量名\n",
    "        pred_var_list = []\n",
    "        for ele in sorted_list:\n",
    "            var_index = ele[0]\n",
    "            pred_var_list.append(var_name_list[var_index])\n",
    "        recommendations_list.append(pred_var_list)\n",
    "\n",
    "        # 从 pred_var_list 中取跟日志中记录变量个数一样的前几个预测变量\n",
    "        limit_pred_var_list = pred_var_list[0:len(ground_truth_list)]\n",
    "        # 计算模型预测的acc\n",
    "        correct_count = 0\n",
    "        total_count = len(ground_truth_list)  # 日志中记录的变量个数\n",
    "        for pred_var in limit_pred_var_list:  # 遍历 pred_var_list 中每个变量\n",
    "            if pred_var in ground_truth_list:\n",
    "                correct_count += 1\n",
    "        sample_acc = correct_count / total_count\n",
    "        # 添加模型预测的准确率\n",
    "        test_acc_list.append(sample_acc)\n",
    "\n",
    "        # 计算当前样本下的mrr\n",
    "        mrr_result = calculate_mrr(ground_truth_list, pred_var_list)\n",
    "        test_mrr_list.append(mrr_result)\n",
    "\n",
    "        # 计算当前样本的top1_acc\n",
    "        top1_result = top_k_acc(ground_truth_list, pred_var_list, k=1)\n",
    "        test_top1_list.append(top1_result)\n",
    "\n",
    "        # 计算当前样本的top2_acc\n",
    "        top2_result = top_k_acc(ground_truth_list, pred_var_list, k=2)\n",
    "        test_top2_list.append(top2_result)\n",
    "\n",
    "# 计算模型的在测试集上整体的 mrr\n",
    "test_mrr = sum(test_mrr_list) / len(test_mrr_list)\n",
    "print(\"test_mrr:\", test_mrr)\n",
    "\n",
    "# 计算top1_acc\n",
    "test_top1_acc = sum(test_top1_list) / len(test_top1_list)\n",
    "print(\"test_top1_acc:\", test_top1_acc)\n",
    "\n",
    "# 计算top2_acc\n",
    "test_top2_acc = sum(test_top2_list) / len(test_top2_list)\n",
    "print(\"test_top2_acc:\", test_top2_acc)\n",
    "\n",
    "# 计算map\n",
    "test_map = calculate_map(recommendations_list, true_logged_variables_list)\n",
    "print(\"test_map:\", test_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
