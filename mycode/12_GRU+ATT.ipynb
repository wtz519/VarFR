{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf4f287-e14e-45e6-bd86-31251b362959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb75cf99-839d-44fe-afca-a50cb6280dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置种子\n",
    "SEED = 5\n",
    "random.seed(SEED)        # Python的随机库\n",
    "np.random.seed(SEED)     # NumPy库\n",
    "torch.manual_seed(SEED)  # CPU上的PyTorch操作\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)           # 当前GPU上的PyTorch操作\n",
    "    torch.cuda.manual_seed_all(SEED)       # 所有GPU上的PyTorch操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8516018-de06-406a-9517-b2b2ce0eeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(recommended_variables, k, true_logged_variables):\n",
    "    relevant = [var for var in recommended_variables[:k] if var in true_logged_variables]\n",
    "    precision = len(relevant) / k\n",
    "    return precision\n",
    "\n",
    "def calculate_average_precision(recommended_variables, true_logged_variables):\n",
    "    precisions = [calculate_precision_at_k(recommended_variables, k + 1, true_logged_variables)\n",
    "                  for k in range(len(recommended_variables)) if recommended_variables[k] in true_logged_variables]\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    average_precision = sum(precisions) / len(true_logged_variables)\n",
    "    return average_precision\n",
    "\n",
    "def calculate_map(recommendations, true_logged_variables_list):\n",
    "    average_precisions = [calculate_average_precision(recs, true_vars)\n",
    "                          for recs, true_vars in zip(recommendations, true_logged_variables_list)]\n",
    "    map_score = sum(average_precisions) / len(average_precisions)\n",
    "    return map_score\n",
    "\n",
    "def top_k_acc(ground_truth_list, pred_list, k):\n",
    "    # 取前k个预测结果\n",
    "    pred_top_k_list = pred_list[0:k]\n",
    "    # 遍历 pred_top_k_list 查询预测的变量是否在ground_truth_list中\n",
    "    for pred_var1 in pred_top_k_list:\n",
    "        if pred_var1 in ground_truth_list:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def calculate_mrr(true_labels, predicted_lst):\n",
    "    \"\"\" 计算 MRR \"\"\"\n",
    "    reciprocal_rank_list = []\n",
    "    # 查询 true_labels 中每个变量在 predicted_lst 中的位置\n",
    "    for var_name in true_labels:\n",
    "        # 如果变量在预测的列表里\n",
    "        if var_name in predicted_lst:\n",
    "            # 查询 var_name 在 predicted_lst 列表中的第一个位置\n",
    "            first_occurrence_index = predicted_lst.index(var_name)\n",
    "            # 求该变量索引的倒数值\n",
    "            var_result = 1 / (first_occurrence_index + 1)\n",
    "            # print(\"var_result:\", var_result)\n",
    "            reciprocal_rank_list.append(var_result)\n",
    "        # 如果变量不在预测的列表中\n",
    "        elif var_name not in predicted_lst:\n",
    "            var_result = 0\n",
    "            reciprocal_rank_list.append(var_result)\n",
    "    return max(reciprocal_rank_list)\n",
    "\n",
    "def find_top_three_indices(lst):\n",
    "    # 创建列表,其中列表的元素是元组，每个元组包含预测值和对应的索引\n",
    "    indexed_lst = [(index1, value) for index1, value in enumerate(lst)]\n",
    "    # print(\"排序前:\", indexed_lst)\n",
    "    # 对indexed_lst 按每个元组的第1个子元素从大到小顺序排序\n",
    "    indexed_lst.sort(reverse=True, key=lambda x: x[1])\n",
    "    # print(\"排序后:\", indexed_lst)\n",
    "    return indexed_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba78d8d-4254-4d1b-be1b-846c48a7245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MaskedBCEWithLogitsLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets, mask):\n",
    "        # 应用 Sigmoid 激活函数\n",
    "        logits = torch.sigmoid(logits)\n",
    "\n",
    "        # 计算二进制交叉熵损失\n",
    "        loss = - (targets * torch.log(logits + 1e-7) + (1 - targets) * torch.log(1 - logits + 1e-7))\n",
    "\n",
    "        # 将损失张量与掩码相乘以过滤掉不想考虑的样本的损失\n",
    "        loss = loss * mask\n",
    "        if self.reduction == 'mean':\n",
    "            # 计算平均损失\n",
    "            loss = torch.sum(loss) / torch.sum(mask)\n",
    "        elif self.reduction == 'sum':\n",
    "            # 计算总损失\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fffc13-0ec6-4bf6-9f68-0c5929c9801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pkl_path):\n",
    "        # 加载pkl文件\n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            pkl_data_list = pickle.load(file)\n",
    "        self.pkl_data_list = pkl_data_list\n",
    "        self.pkl_data_len = len(self.pkl_data_list)\n",
    "        print(\"pkl_path:\", pkl_path)\n",
    "        # print(\"pkl_data_len:\", self.pkl_data_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pkl_data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取第 index 个样本\n",
    "        train_data = self.pkl_data_list[idx]\n",
    "        # print(train_data)\n",
    "        return (train_data['token_embedding_tensor'], train_data['token_label_tensor'],\n",
    "                train_data['token_label_mask_tensor'], train_data['var_location'],\n",
    "                train_data['train_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971f9791-9b98-4a5b-87e6-0d4420cd6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl_path: ./token_split_data/token_train_pkl.pkl\n",
      "pkl_path: ./token_split_data/token_val_pkl.pkl\n",
      "pkl_path: ./token_split_data/token_test_pkl.pkl\n",
      "train_dataset_size: 647\n",
      "val_dataset_size: 80\n",
      "test_dataset_size: 82\n",
      "total_data_size: 809\n"
     ]
    }
   ],
   "source": [
    "# 实例化数据集对象\n",
    "train_dataset = LstmDataset(\"./token_split_data/token_train_pkl.pkl\")\n",
    "val_dataset = LstmDataset(\"./token_split_data/token_val_pkl.pkl\")\n",
    "test_dataset = LstmDataset(\"./token_split_data/token_test_pkl.pkl\")\n",
    "\n",
    "print(\"train_dataset_size:\", len(train_dataset))\n",
    "print(\"val_dataset_size:\", len(val_dataset))\n",
    "print(\"test_dataset_size:\", len(test_dataset))\n",
    "print(\"total_data_size:\", len(train_dataset)+len(val_dataset)+len(test_dataset))\n",
    "\n",
    "# 加载数据集, 构造 dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76fa323b-ad56-4d3b-aaee-0d81ab8402a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU_with_Att(\n",
       "  (GRU): GRU(100, 128, batch_first=True, bidirectional=True)\n",
       "  (multi_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (classifier): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建网络模型\n",
    "class GRU_with_Att(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.GRU = nn.GRU(input_size=100,\n",
    "                          hidden_size=128,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.multi_att = nn.MultiheadAttention(num_heads=4, embed_dim=256)\n",
    "        self.linear1 = nn.Linear(2 * 128, 1)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GRU层\n",
    "        output, h = self.GRU(x)\n",
    "        # print(\"  GRU output.shape:\", output.shape)\n",
    "        # print(\"  GRU hidden.shape:\", h.shape)\n",
    "\n",
    "        # 多头注意力层\n",
    "        output, attn_weights = self.multi_att(output, output, output)\n",
    "        # print(\"  multi-head output Shape:\", output.shape)\n",
    "        # print(\"  attention Weights Shape:\", attn_weights.shape)\n",
    "\n",
    "        # sigmoid 层\n",
    "        output = self.classifier(self.linear1(output))\n",
    "        output = output.squeeze(-1)\n",
    "        return output\n",
    "\n",
    "# 模型实例化\n",
    "model1 = GRU_with_Att()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a154e1c-7460-4be4-82c8-5da3b6f65abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型测试:\n",
      "test_tensor output.shape: torch.Size([80, 512])\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "print(\"模型测试:\")\n",
    "test_tensor = torch.randn(80, 512, 100, device=device)\n",
    "output = model1(test_tensor)\n",
    "print(\"test_tensor output.shape:\", output.shape)  # torch.Size([80, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c959d3-d109-465f-a208-2c112a55868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失和优化器\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac85807c-7d7a-43cd-8131-c29d9fbb50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 第 0 轮训练开始 ============\n",
      "train_loss_sum: 7.43703556060791\n",
      "val_loss_sum: 50.3332696557045\n",
      "val_acc: 0.4611309523809525\n",
      "val_mrr: 0.6619047619047621\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.675\n",
      "val_map: 0.6192786115393127\n",
      "模型已经保存\n",
      "============ 第 1 轮训练开始 ============\n",
      "train_loss_sum: 6.427145600318909\n",
      "val_loss_sum: 41.82658088207245\n",
      "val_acc: 0.5257142857142857\n",
      "val_mrr: 0.668779761904762\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6432148619208755\n",
      "模型已经保存\n",
      "============ 第 2 轮训练开始 ============\n",
      "train_loss_sum: 5.062447428703308\n",
      "val_loss_sum: 30.197501093149185\n",
      "val_acc: 0.49706845238095243\n",
      "val_mrr: 0.6459429112554114\n",
      "val_top1_acc: 0.475\n",
      "val_top2_acc: 0.6625\n",
      "val_map: 0.629784362732564\n",
      "============ 第 3 轮训练开始 ============\n",
      "train_loss_sum: 3.2786076962947845\n",
      "val_loss_sum: 16.872429341077805\n",
      "val_acc: 0.4725892857142858\n",
      "val_mrr: 0.6358387445887448\n",
      "val_top1_acc: 0.45\n",
      "val_top2_acc: 0.675\n",
      "val_map: 0.6173481852968462\n",
      "============ 第 4 轮训练开始 ============\n",
      "train_loss_sum: 1.562787376344204\n",
      "val_loss_sum: 6.708835564553738\n",
      "val_acc: 0.4663392857142858\n",
      "val_mrr: 0.6381398809523812\n",
      "val_top1_acc: 0.4625\n",
      "val_top2_acc: 0.6625\n",
      "val_map: 0.6141672689909299\n",
      "============ 第 5 轮训练开始 ============\n",
      "train_loss_sum: 0.5569434054195881\n",
      "val_loss_sum: 2.3822282180190086\n",
      "val_acc: 0.4663392857142858\n",
      "val_mrr: 0.6419990079365081\n",
      "val_top1_acc: 0.4625\n",
      "val_top2_acc: 0.6875\n",
      "val_map: 0.6205870644732253\n",
      "============ 第 6 轮训练开始 ============\n",
      "train_loss_sum: 0.22364435996860266\n",
      "val_loss_sum: 1.2992480238899589\n",
      "val_acc: 0.4830059523809525\n",
      "val_mrr: 0.6572767857142858\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6331600036867895\n",
      "============ 第 7 轮训练开始 ============\n",
      "train_loss_sum: 0.14837116282433271\n",
      "val_loss_sum: 1.0716394330374897\n",
      "val_acc: 0.4845684523809525\n",
      "val_mrr: 0.6480208333333335\n",
      "val_top1_acc: 0.475\n",
      "val_top2_acc: 0.6875\n",
      "val_map: 0.6260958915836148\n",
      "============ 第 8 轮训练开始 ============\n",
      "train_loss_sum: 0.1265297131612897\n",
      "val_loss_sum: 0.993743282277137\n",
      "val_acc: 0.4824851190476191\n",
      "val_mrr: 0.6581250000000001\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.630426744758218\n",
      "============ 第 9 轮训练开始 ============\n",
      "train_loss_sum: 0.11632001493126154\n",
      "val_loss_sum: 0.9347857404500246\n",
      "val_acc: 0.4897767857142858\n",
      "val_mrr: 0.6583035714285715\n",
      "val_top1_acc: 0.475\n",
      "val_top2_acc: 0.7\n",
      "val_map: 0.6315948995201228\n",
      "============ 第 10 轮训练开始 ============\n",
      "train_loss_sum: 0.10748200863599777\n",
      "val_loss_sum: 0.8773933304473758\n",
      "val_acc: 0.5210267857142858\n",
      "val_mrr: 0.6812202380952382\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.7375\n",
      "val_map: 0.6543081931709165\n",
      "模型已经保存\n",
      "============ 第 11 轮训练开始 ============\n",
      "train_loss_sum: 0.09893970470875502\n",
      "val_loss_sum: 0.8197209355421364\n",
      "val_acc: 0.519360119047619\n",
      "val_mrr: 0.6939285714285715\n",
      "val_top1_acc: 0.5125\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6580095820598055\n",
      "模型已经保存\n",
      "============ 第 12 轮训练开始 ============\n",
      "train_loss_sum: 0.0930394004099071\n",
      "val_loss_sum: 0.7635917789302766\n",
      "val_acc: 0.5422767857142856\n",
      "val_mrr: 0.7325389194139195\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6813483717423453\n",
      "模型已经保存\n",
      "============ 第 13 轮训练开始 ============\n",
      "train_loss_sum: 0.08731341594830155\n",
      "val_loss_sum: 0.7087216189829633\n",
      "val_acc: 0.5374851190476189\n",
      "val_mrr: 0.732624458874459\n",
      "val_top1_acc: 0.575\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6866081344469608\n",
      "模型已经保存\n",
      "============ 第 14 轮训练开始 ============\n",
      "train_loss_sum: 0.08996903896331787\n",
      "val_loss_sum: 0.6545814977725968\n",
      "val_acc: 0.557797619047619\n",
      "val_mrr: 0.748154761904762\n",
      "val_top1_acc: 0.6\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.7040957148642286\n",
      "模型已经保存\n",
      "============ 第 15 轮训练开始 ============\n",
      "train_loss_sum: 0.07521313522011042\n",
      "val_loss_sum: 0.602781334426254\n",
      "val_acc: 0.5541517857142856\n",
      "val_mrr: 0.7388541666666667\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.7008818602251508\n",
      "============ 第 16 轮训练开始 ============\n",
      "train_loss_sum: 0.07446260564029217\n",
      "val_loss_sum: 0.5648936287034303\n",
      "val_acc: 0.5417559523809523\n",
      "val_mrr: 0.7363541666666669\n",
      "val_top1_acc: 0.5875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.690081253713607\n",
      "============ 第 17 轮训练开始 ============\n",
      "train_loss_sum: 0.07418903708457947\n",
      "val_loss_sum: 0.5557879426050931\n",
      "val_acc: 0.555922619047619\n",
      "val_mrr: 0.72484126984127\n",
      "val_top1_acc: 0.5625\n",
      "val_top2_acc: 0.8\n",
      "val_map: 0.6918596906565659\n",
      "============ 第 18 轮训练开始 ============\n",
      "train_loss_sum: 0.0651713041588664\n",
      "val_loss_sum: 0.5991921563399956\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.7074107142857144\n",
      "val_top1_acc: 0.5375\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6795034184565437\n",
      "============ 第 19 轮训练开始 ============\n",
      "train_loss_sum: 0.06653793668374419\n",
      "val_loss_sum: 0.6635425614658743\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.7074107142857144\n",
      "val_top1_acc: 0.5375\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6815371486152738\n",
      "============ 第 20 轮训练开始 ============\n",
      "train_loss_sum: 0.07100457511842251\n",
      "val_loss_sum: 0.6872085071518086\n",
      "val_acc: 0.5205059523809523\n",
      "val_mrr: 0.7005357142857145\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6739230613136865\n",
      "============ 第 21 轮训练开始 ============\n",
      "train_loss_sum: 0.06223084218800068\n",
      "val_loss_sum: 0.6982005515601486\n",
      "val_acc: 0.5205059523809523\n",
      "val_mrr: 0.699077380952381\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6721081574675327\n",
      "============ 第 22 轮训练开始 ============\n",
      "train_loss_sum: 0.06093446281738579\n",
      "val_loss_sum: 0.6642987198429182\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.6989637445887447\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6780354246447998\n",
      "============ 第 23 轮训练开始 ============\n",
      "train_loss_sum: 0.0617234637029469\n",
      "val_loss_sum: 0.6629791916930117\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.6989637445887447\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6774104246447998\n",
      "============ 第 24 轮训练开始 ============\n",
      "train_loss_sum: 0.06067618075758219\n",
      "val_loss_sum: 0.683750394557137\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.6989637445887447\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6781865755772007\n",
      "============ 第 25 轮训练开始 ============\n",
      "train_loss_sum: 0.0658580120652914\n",
      "val_loss_sum: 0.7038943641236983\n",
      "val_acc: 0.5330059523809523\n",
      "val_mrr: 0.697922077922078\n",
      "val_top1_acc: 0.525\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6784606140387391\n",
      "============ 第 26 轮训练开始 ============\n",
      "train_loss_sum: 0.060269135516136885\n",
      "val_loss_sum: 0.7020056580659002\n",
      "val_acc: 0.5121726190476189\n",
      "val_mrr: 0.6870887445887448\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.666925891816517\n",
      "============ 第 27 轮训练开始 ============\n",
      "train_loss_sum: 0.06464446848258376\n",
      "val_loss_sum: 0.7018049425678328\n",
      "val_acc: 0.5121726190476189\n",
      "val_mrr: 0.6870887445887448\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6665092251498502\n",
      "============ 第 28 轮训练开始 ============\n",
      "train_loss_sum: 0.059854555409401655\n",
      "val_loss_sum: 0.6978448171284981\n",
      "val_acc: 0.5121726190476189\n",
      "val_mrr: 0.6850054112554114\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.7375\n",
      "val_map: 0.6630370029276281\n",
      "============ 第 29 轮训练开始 ============\n",
      "train_loss_sum: 0.06112412130460143\n",
      "val_loss_sum: 0.6700029841158539\n",
      "val_acc: 0.5199851190476189\n",
      "val_mrr: 0.6808387445887447\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6615015704781332\n",
      "============ 第 30 轮训练开始 ============\n",
      "train_loss_sum: 0.05947626335546374\n",
      "val_loss_sum: 0.6975418674410321\n",
      "val_acc: 0.5158184523809524\n",
      "val_mrr: 0.6793804112554114\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.75\n",
      "val_map: 0.6582724038114665\n",
      "============ 第 31 轮训练开始 ============\n",
      "train_loss_sum: 0.0579927871003747\n",
      "val_loss_sum: 0.7100830177078024\n",
      "val_acc: 0.5220684523809523\n",
      "val_mrr: 0.6835470779220781\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6604511972402599\n",
      "============ 第 32 轮训练开始 ============\n",
      "train_loss_sum: 0.05844936240464449\n",
      "val_loss_sum: 0.7102056553121656\n",
      "val_acc: 0.5283184523809523\n",
      "val_mrr: 0.6863744588744589\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6622121099386725\n",
      "============ 第 33 轮训练开始 ============\n",
      "train_loss_sum: 0.05825810204260051\n",
      "val_loss_sum: 0.7109332334948704\n",
      "val_acc: 0.5283184523809523\n",
      "val_mrr: 0.6864880952380953\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6599178664837148\n",
      "============ 第 34 轮训练开始 ============\n",
      "train_loss_sum: 0.05608838610351086\n",
      "val_loss_sum: 0.7148569415439852\n",
      "val_acc: 0.5283184523809523\n",
      "val_mrr: 0.6927380952380953\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.6620011998170482\n",
      "============ 第 35 轮训练开始 ============\n",
      "train_loss_sum: 0.05453052558004856\n",
      "val_loss_sum: 0.7650300782406703\n",
      "val_acc: 0.5179017857142856\n",
      "val_mrr: 0.6910565476190478\n",
      "val_top1_acc: 0.5\n",
      "val_top2_acc: 0.7875\n",
      "val_map: 0.658496284593383\n",
      "============ 第 36 轮训练开始 ============\n",
      "train_loss_sum: 0.05174444802105427\n",
      "val_loss_sum: 0.7195028471178375\n",
      "val_acc: 0.5116517857142857\n",
      "val_mrr: 0.6806398809523811\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.7625\n",
      "val_map: 0.6532879512600498\n",
      "============ 第 37 轮训练开始 ============\n",
      "train_loss_sum: 0.048112211748957634\n",
      "val_loss_sum: 0.8169900414068252\n",
      "val_acc: 0.5179017857142856\n",
      "val_mrr: 0.6825496031746032\n",
      "val_top1_acc: 0.4875\n",
      "val_top2_acc: 0.775\n",
      "val_map: 0.6537125093408578\n",
      "早停: 在连续 20 个 epoch 中验证集损失loss没有改善.\n"
     ]
    }
   ],
   "source": [
    "# 模型训练和测试\n",
    "if __name__ == '__main__':\n",
    "    epoch_list = []\n",
    "    epoch_train_loss_sum_list = []\n",
    "    epoch_val_acc_list = []  # 统计所有 epoch 下 val 集的平均acc\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # 早停的参数\n",
    "    min_val_loss_sum = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    patience = 20  # 设置早停等待的 epoch 数\n",
    "\n",
    "    for epoch in range(200):\n",
    "        # 模型训练\n",
    "        model1.train()\n",
    "        train_loss_sum = 0\n",
    "        print(f\"============ 第 {epoch} 轮训练开始 ============\")\n",
    "        # 记录当前的epoch\n",
    "        epoch_list.append(epoch)\n",
    "        for batch_index, data in enumerate(train_loader):\n",
    "            # print(\"batch_index:\", batch_index)\n",
    "            # print(\"all_data:\")\n",
    "            # print(all_data)\n",
    "            # print(\"len(all_data):\", len(all_data))\n",
    "\n",
    "            token_inputs, token_targets, token_targets_mask, var_locations, train_names = data\n",
    "            token_inputs, token_targets, token_targets_mask = token_inputs.to(device), token_targets.to(device), token_targets_mask.to(device)\n",
    "            # print(\"inputs.shape:\", token_inputs.shape)  # [16, 550, 768]\n",
    "            # print(\"targets.shape:\", token_targets.shape)  # [16, 20]\n",
    "            # print(\"targets_mask.shape:\", token_targets_mask.shape)\n",
    "            # print(\"var_location.shape:\", var_locations.shape)\n",
    "            # print(\"len(train_names):\", len(train_names))\n",
    "\n",
    "            # 优化器清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model1(token_inputs)\n",
    "            # print(\"outputs.shape:\", outputs.shape)\n",
    "            loss = criterion(outputs, token_targets)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "\n",
    "            # update\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item()\n",
    "        print(\"train_loss_sum:\", train_loss_sum)\n",
    "        \n",
    "        # 记录当前epoch下的 current_train_loss_sum\n",
    "        epoch_train_loss_sum_list.append(train_loss_sum)\n",
    "\n",
    "        # 模型验证 (评估当前训练模型的性能)\n",
    "        with torch.no_grad():\n",
    "            model1.eval()\n",
    "            val_acc_list = []  # 统计当前 epoch 中每个样本的准确率\n",
    "            val_mrr_list = []\n",
    "            val_top1_list = []\n",
    "            val_top2_list = []\n",
    "            recommendations_list = []\n",
    "            true_logged_variables_list = []\n",
    "            val_loss_sum = 0\n",
    "            for batch_index, data in enumerate(val_loader):\n",
    "                val_inputs, val_target, val_target_mask, val_var_location, val_name_list = data\n",
    "                val_inputs, val_target, val_target_mask = val_inputs.to(device), val_target.to(device), val_target_mask.to(device)\n",
    "\n",
    "                # 获取测试的方法的名字\n",
    "                val_file_name = val_name_list[0]\n",
    "                # print(\"val_name_list:\", val_name_list)\n",
    "                # print(\"val_file_name:\", val_file_name)\n",
    "                # print(\"val_inputs.shape:\", val_inputs.shape)\n",
    "                # print(\"val_target:\", val_target.shape)\n",
    "                # print(\"val_target_mask:\", val_target_mask.shape)\n",
    "                # print(\"var_location:\", val_var_location.shape)\n",
    "\n",
    "                outputs = model1(val_inputs)  # [1, 5664]\n",
    "                # print(\"val_outputs:\", outputs)\n",
    "                # print(\"val_outputs.shape:\", outputs.shape)\n",
    "                loss = criterion(outputs, val_target)\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                # 读取 token_text 文件夹下的方法, 获取每个位置所对应的 token\n",
    "                token_text_dir = \"./token_text\"\n",
    "                token_json_name = val_file_name.split(\".txt\")[0] + \".json\"\n",
    "                token_text_path = os.path.join(token_text_dir, token_json_name)\n",
    "                with open(token_text_path, 'r') as json_file:\n",
    "                    # 加载 JSON 数据\n",
    "                    token_list = json.load(json_file)\n",
    "                # print(\"token_list:\", token_list)\n",
    "                new_token_list = []\n",
    "                for one in token_list:\n",
    "                    # print(one[\"token\"])\n",
    "                    new_token_list.append(one[\"token\"])\n",
    "\n",
    "                # 取 output 里的前 token_len 个有效的预测结果\n",
    "                token_len = len(new_token_list)\n",
    "                outputs_list1 = outputs.squeeze().tolist()[0:token_len]\n",
    "                # print(\"未截取前outputs长度:\", len(outputs.squeeze().tolist()))\n",
    "                # print(\"截取后的outputs长度:\", len(outputs_list1))\n",
    "                # print(\"new_token_list长度:\", len(new_token_list))\n",
    "                # print(\"target_mask中1的个数为:\", val_target_mask.squeeze().tolist().count(1))\n",
    "\n",
    "                # 使用 zip 将 token 跟每个 token 的预测结果进行匹配\n",
    "                pred_list = list(zip(new_token_list, outputs_list1))\n",
    "                # print(\"pred_list:\", pred_list)\n",
    "\n",
    "                # 根据 var_location 取 output 中有效的预测结果\n",
    "                val_var_location = val_var_location.squeeze().tolist()\n",
    "                # print(\"val_var_location:\", val_var_location)\n",
    "\n",
    "                sorted_pred_list = []\n",
    "                # 根据 var_location为 1 的位置, 取出 pred_list 中对应位置的元素\n",
    "                for token_index, token_label in enumerate(val_var_location):\n",
    "                    if token_label == 1:\n",
    "                        sorted_pred_list.append(pred_list[token_index])\n",
    "                # print(\"未排序 sorted_pred_list:\", sorted_pred_list)\n",
    "                # 按照概率值大小对预测结果进行排序\n",
    "                sorted_pred_list.sort(reverse=True, key=lambda x: x[1])\n",
    "                # print(\"排序后 sorted_pred_list:\", sorted_pred_list)\n",
    "\n",
    "                # 读取 all_label文件夹下的label文件,获取导致bug的变量名\n",
    "                label_dir = \"./all_label\"\n",
    "                label_path = os.path.join(label_dir, val_file_name.split(\".txt\")[0]+\".csv\")\n",
    "                label_csv = pd.read_csv(label_path)\n",
    "                # var_name_list 列表\n",
    "                var_name_list = label_csv['name'].tolist()\n",
    "                # print(\"var_name_list:\", var_name_list)\n",
    "                # all_label 列表\n",
    "                label_list = label_csv['label'].tolist()\n",
    "                # print(\"label_list:\", label_list)\n",
    "\n",
    "                ground_truth_list = []\n",
    "                for index0, label in enumerate(label_list):\n",
    "                    if label == 1:\n",
    "                        ground_truth_list.append(var_name_list[index0])\n",
    "                # print(\"ground_truth_list:\", ground_truth_list)\n",
    "                true_logged_variables_list.append(ground_truth_list)\n",
    "\n",
    "                # 根据模型预测结果, 取出对应的预测的变量名\n",
    "                pred_var_name_list = []\n",
    "                # print(\"概率从大到小排序后的局部变量推荐表:\")\n",
    "                for ele in sorted_pred_list:\n",
    "                    var_name = ele[0]  # 取出变量名\n",
    "                    # 不添加同名变量\n",
    "                    if var_name not in pred_var_name_list:\n",
    "                        pred_var_name_list.append(var_name)\n",
    "                # print(\"pred_var_name_list:\", pred_var_name_list)\n",
    "                recommendations_list.append(pred_var_name_list)\n",
    "\n",
    "                # 从 pred_var_name_list 中取跟 ground_truth_list 变量个数一样的前几个预测变量\n",
    "                limit_pred_var_name_list = pred_var_name_list[0:len(ground_truth_list)]\n",
    "                # print(\"limit_pred_var_name_list:\", limit_pred_var_name_list)\n",
    "                # print(\"ground_truth_list :\", ground_truth_list)\n",
    "\n",
    "                # 计算模型预测的正确率\n",
    "                correct_count = 0\n",
    "                total_count = len(ground_truth_list)  # 日志中记录的变量个数\n",
    "                for pred_var_name in limit_pred_var_name_list:  # 遍历 pred_var_name_list 中每个变量名\n",
    "                    if pred_var_name in ground_truth_list:\n",
    "                        correct_count += 1\n",
    "                sample_acc = correct_count / total_count\n",
    "\n",
    "                # 添加模型预测的准确率\n",
    "                val_acc_list.append(sample_acc)\n",
    "\n",
    "                # 计算当前样本的 mrr\n",
    "                mrr_result = calculate_mrr(ground_truth_list, pred_var_name_list)\n",
    "                val_mrr_list.append(mrr_result)\n",
    "\n",
    "                # 计算当前样本的top1_acc\n",
    "                top1_result = top_k_acc(ground_truth_list, pred_var_name_list, k=1)\n",
    "                val_top1_list.append(top1_result)\n",
    "\n",
    "                # 计算当前样本的top2_acc\n",
    "                top2_result = top_k_acc(ground_truth_list, pred_var_name_list, k=2)\n",
    "                val_top2_list.append(top2_result)\n",
    "\n",
    "        # 当前 epoch 下的 loss_sum\n",
    "        print(\"val_loss_sum:\", val_loss_sum)\n",
    "\n",
    "        # 计算当前epoch下,模型在验证集下的平均预测准确率\n",
    "        current_epoch_average_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        print(\"val_acc:\", current_epoch_average_acc)\n",
    "        # epoch_val_acc_list.append(current_epoch_average_acc)\n",
    "\n",
    "        # 输出当前epoch下的 val_mrr\n",
    "        val_mrr = sum(val_mrr_list) / len(val_mrr_list)\n",
    "        print(\"val_mrr:\", val_mrr)\n",
    "        #\n",
    "        # 计算当前epoch下,模型的top1_acc\n",
    "        val_top1_acc = sum(val_top1_list) / len(val_top1_list)\n",
    "        print(\"val_top1_acc:\", val_top1_acc)\n",
    "        #\n",
    "        # 计算当前epoch下,模型的top2_acc\n",
    "        val_top2_acc = sum(val_top2_list) / len(val_top2_list)\n",
    "        print(\"val_top2_acc:\", val_top2_acc)\n",
    "\n",
    "        # 计算当前epoch下,模型 map\n",
    "        val_map = calculate_map(recommendations_list, true_logged_variables_list)\n",
    "        print(\"val_map:\", val_map)\n",
    "\n",
    "        # 检查val_loss_sum是否有改善\n",
    "        if val_loss_sum < min_val_loss_sum:\n",
    "            min_val_loss_sum = val_loss_sum\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # 检查是否需要早停\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"早停: 在连续 {patience} 个 epoch 中验证集损失loss没有改善.\")\n",
    "            break\n",
    "\n",
    "        # Save the model if the accuracy is the best\n",
    "        if val_map > best_accuracy:\n",
    "            # 模型保存\n",
    "            model_dir = \"./model_save\"\n",
    "            model_name = \"GRU_model.pth\"\n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            torch.save(model1, model_path)\n",
    "            best_accuracy = val_map\n",
    "            print(\"模型已经保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87da73da-4dc8-4cf9-9692-9636581241e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============模型开始测试=============\n",
      "test_mrr: 0.6794961813254496\n",
      "test_top1_acc: 0.4878048780487805\n",
      "test_top2_acc: 0.7560975609756098\n",
      "test_map: 0.6448227483898218\n"
     ]
    }
   ],
   "source": [
    "print(\"=============模型开始测试=============\")\n",
    "# 模型测试\n",
    "test_model = torch.load(\"./model_save/GRU_model.pth\")\n",
    "test_model.eval()\n",
    "test_acc_list = []  # 统计每个样本的准确率\n",
    "test_mrr_list = []\n",
    "test_top1_list = []\n",
    "test_top2_list = []\n",
    "true_logged_variables_list = []\n",
    "recommendations_list = []\n",
    "with torch.no_grad():\n",
    "    for batch_index, data in enumerate(test_loader):\n",
    "        test_inputs, test_target, test_target_mask, test_var_location, test_name_list = data\n",
    "        test_inputs, test_target, test_target_mask = test_inputs.to(device), test_target.to(device), test_target_mask.to(device)\n",
    "        # print()\n",
    "        # print(f\"batch_{batch_index}:\")\n",
    "        # 获取测试的方法的 has_log_method_csv 名字\n",
    "        test_file_name = test_name_list[0]\n",
    "        # print(\"test_csv_name:\", csv_name)\n",
    "        # print(\"test_inputs.shape:\", test_inputs.shape)\n",
    "        # print(\"test_target:\", test_target.shape)\n",
    "        # print(\"test_target_mask:\", test_target_mask.shape)\n",
    "        # print(\"test_var_location:\", test_var_location.shape)\n",
    "\n",
    "        outputs = model1(test_inputs)  # [1, 5664]\n",
    "        # print(\"val_outputs:\", outputs)\n",
    "        # print(\"test_outputs.shape:\", outputs.shape)\n",
    "        loss = criterion(outputs, test_target)\n",
    "\n",
    "        # 读取 token_text文件夹下的方法体,获取每个位置的token\n",
    "        token_text_dir = \"./token_text\"\n",
    "        token_json_name = test_file_name.split(\".txt\")[0] + \".json\"\n",
    "        token_text_path = os.path.join(token_text_dir, token_json_name)\n",
    "        with open(token_text_path, 'r') as json_file:\n",
    "            # 加载 JSON 数据\n",
    "            token_list = json.load(json_file)\n",
    "        # print(\"token_list:\", token_list)\n",
    "        new_token_list = []\n",
    "        for one in token_list:\n",
    "            # print(one[\"token\"])\n",
    "            new_token_list.append(one[\"token\"])\n",
    "\n",
    "        # 取 output 里的前 token_len 个有效的预测结果\n",
    "        token_len = len(new_token_list)\n",
    "        outputs_list1 = outputs.squeeze().tolist()[0:token_len]\n",
    "        # print(\"未截取前outputs长度:\", len(outputs.squeeze().tolist()))\n",
    "        # print(\"截取后的outputs长度:\", len(outputs_list1))\n",
    "        # print(\"new_token_list长度:\", len(new_token_list))\n",
    "        # print(\"target_mask中1的个数为:\", val_target_mask.squeeze().tolist().count(1))\n",
    "\n",
    "        # 使用 zip 将token 跟每个token的预测结果进行匹配\n",
    "        pred_list = list(zip(new_token_list, outputs_list1))\n",
    "        # print(\"pred_list:\", pred_list)\n",
    "\n",
    "        # 根据 var_location 取 output 中有效的预测结果\n",
    "        test_var_location = test_var_location.squeeze().tolist()\n",
    "        # print(\"val_var_location:\", val_var_location)\n",
    "\n",
    "        sorted_pred_list = []\n",
    "        # 根据 var_location为1的位置, 取出pred_list中对应位置的元素\n",
    "        for token_index, token_label in enumerate(test_var_location):\n",
    "            if token_label == 1:\n",
    "                sorted_pred_list.append(pred_list[token_index])\n",
    "        # print(\"未排序 sorted_pred_list:\", sorted_pred_list)\n",
    "        # 按照概率值大小对预测结果进行排序\n",
    "        sorted_pred_list.sort(reverse=True, key=lambda x: x[1])\n",
    "        # print(\"排序后 sorted_pred_list:\", sorted_pred_list)\n",
    "\n",
    "        # 读取 train_label 获取日志中记录的变量名\n",
    "        label_dir = \"./all_label\"\n",
    "        label_path = os.path.join(label_dir, test_file_name.split(\".txt\")[0]+\".csv\")\n",
    "        label_csv = pd.read_csv(label_path)\n",
    "        # var_name_list 列表\n",
    "        var_name_list = label_csv['name'].tolist()\n",
    "        # print(\"var_name_list:\", var_name_list)\n",
    "        # all_label 列表\n",
    "        label_list = label_csv['label'].tolist()\n",
    "        # print(\"label_list:\", label_list)\n",
    "\n",
    "        # 根据 label_list 获得日志中记录的变量名\n",
    "        ground_truth_list = []\n",
    "        for index0, label in enumerate(label_list):\n",
    "            if label == 1:\n",
    "                ground_truth_list.append(var_name_list[index0])\n",
    "        # print(\"ground_truth_list:\", ground_truth_list)\n",
    "        true_logged_variables_list.append(ground_truth_list)\n",
    "\n",
    "        # 根据模型预测结果,取出对应的预测的变量名\n",
    "        pred_var_name_list = []\n",
    "        # print(\"概率从大到小排序后的局部变量推荐表:\")\n",
    "        for ele in sorted_pred_list:\n",
    "            var_name = ele[0]  # 取出变量名\n",
    "            # 不添加同名变量\n",
    "            if var_name not in pred_var_name_list:\n",
    "                pred_var_name_list.append(var_name)\n",
    "        # print(\"pred_var_name_list:\", pred_var_name_list)\n",
    "        recommendations_list.append(pred_var_name_list)\n",
    "\n",
    "        # 从 pred_var_name_list 中取跟 ground_truth_list 变量个数一样的前几个预测变量\n",
    "        limit_pred_var_name_list = pred_var_name_list[0:len(ground_truth_list)]\n",
    "        # print(\"pred_var_name_list:\", pred_var_name_list)\n",
    "        # print(\"ground_truth_list :\", ground_truth_list)\n",
    "\n",
    "        # 计算模型预测的 acc\n",
    "        correct_count = 0\n",
    "        total_count = len(ground_truth_list)  # 日志中记录的变量个数\n",
    "        for pred_var_name in limit_pred_var_name_list:  # 遍历 pred_var_name_list 中每个变量名\n",
    "            if pred_var_name in ground_truth_list:\n",
    "                correct_count += 1\n",
    "        sample_acc = correct_count / total_count\n",
    "        test_acc_list.append(sample_acc)\n",
    "\n",
    "        # 计算当前样本的 mrr\n",
    "        mrr_result = calculate_mrr(ground_truth_list, pred_var_name_list)\n",
    "        test_mrr_list.append(mrr_result)\n",
    "\n",
    "        # 计算当前样本的top1_acc\n",
    "        top1_result = top_k_acc(ground_truth_list, pred_var_name_list, k=1)\n",
    "        test_top1_list.append(top1_result)\n",
    "\n",
    "        # 计算当前样本的top2_acc\n",
    "        top2_result = top_k_acc(ground_truth_list, pred_var_name_list, k=2)\n",
    "        test_top2_list.append(top2_result)\n",
    "\n",
    "# # 模型在测试集下的平均预测acc\n",
    "# current_epoch_average_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "# print(\"test_acc:\", current_epoch_average_acc)\n",
    "\n",
    "# 模型在测试集下的平均预测mrr\n",
    "test_mrr = sum(test_mrr_list) / len(test_mrr_list)\n",
    "print(\"test_mrr:\", test_mrr)\n",
    "\n",
    "# 计算top1_acc\n",
    "test_top1_acc = sum(test_top1_list) / len(test_top1_list)\n",
    "print(\"test_top1_acc:\", test_top1_acc)\n",
    "\n",
    "# 计算top2_acc\n",
    "test_top2_acc = sum(test_top2_list) / len(test_top2_list)\n",
    "print(\"test_top2_acc:\", test_top2_acc)\n",
    "\n",
    "# 计算 map\n",
    "test_map = calculate_map(recommendations_list, true_logged_variables_list)\n",
    "print(\"test_map:\", test_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
